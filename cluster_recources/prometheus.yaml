apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: prometheus
  annotations:
    "openshift.io/display-name": Prometheus
    description: |
      A monitoring solution for an OpenShift cluster - collect and gather metrics and alerts from nodes, services, and the infrastructure. This is a tech preview feature.
    iconClass: fa fa-cogs
    tags: "monitoring,prometheus, alertmanager,time-series"
parameters:
- description: The namespace to instantiate prometheus under. Defaults to 'kube-system'.
  name: NAMESPACE
  # value: openshift-metrics
- description: The location of the proxy image
  name: IMAGE_PROXY
  value: openshift/oauth-proxy:v1.1.0
- description: The location of the prometheus image
  name: IMAGE_PROMETHEUS
  value: openshift/prometheus:v2.5.0
- description: The session secret for the proxy
  name: SESSION_SECRET
  generate: expression
  from: "[a-zA-Z0-9]{43}"
- description: Storage type.
  displayName: Starage type
  name: STORAGE_ACCSESS_MODE
  value: "ReadWriteOnce"
- description: Storage size.
  displayName: Jenkins Starage
  name: STORAGE_SIZE
  value: "10Gi"

objects:

#Create Project Monioring
- apiVersion: project.openshift.io/v1
  kind: Project
  metadata:
    annotations:
      openshift.io/description: ""
      openshift.io/display-name: ""
    name: ${NAMESPACE}
  spec:
    finalizers:
    - kubernetes
  status:
    phase: Active

# Authorize the prometheus service account to read data about the cluster
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
    annotations:
      # serviceaccounts.openshift.io/oauth-redirectreference.primary: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"prometheus"}}'
      serviceaccounts.openshift.io/oauth-redirectreference.prom: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"prometheus"}}'
      # serviceaccounts.openshift.io/oauth-redirectreference.alerts: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"alerts"}}'
      # serviceaccounts.openshift.io/oauth-redirectreference.alertmanager: '{"kind":"OAuthRedirectReference","apiVersion":"v1","reference":{"kind":"Route","name":"alertmanager"}}'

# Create a service account for accessing prometheus data
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus-reader
    namespace: "${NAMESPACE}"

# Create a service account for prometheus to use to scrape other infrastructure components
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus-scraper
    namespace: "${NAMESPACE}"

- apiVersion: v1
  kind: Secret
  metadata:
    name: prometheus-scraper
    namespace: "${NAMESPACE}"
    annotations:
      kubernetes.io/service-account.name: prometheus-scraper
  type: kubernetes.io/service-account-token

- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: prometheus-scraper
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  rules:
    - apiGroups:
        - ""
      resources:
        - nodes
        - nodes/metrics
        - routers/metrics
        - services
        - endpoints
        - pods
      verbs:
        - get
        - list
        - watch
    - apiGroups:
        - ""
      resources:
        - configmaps
      verbs:
        - get
    - nonResourceURLs:
        - "/metrics"
      verbs:
        - get

# - apiVersion: rbac.authorization.k8s.io/v1
#   kind: ClusterRole
#   metadata:
#     name: prometheus-scraper
#   rules:
#   - apiGroups:
#     - route.openshift.io
#     resources:
#     - routers/metrics
#     verbs:
#     - get
#   - apiGroups:
#     - image.openshift.io
#     resources:
#     - registry/metrics
#     verbs:
#     - get

- apiVersion: authorization.openshift.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: prometheus-scraper
  roleRef:
    name: prometheus-scraper
  subjects:
  - kind: ServiceAccount
    name: prometheus-scraper
    namespace: "${NAMESPACE}"

- apiVersion: authorization.openshift.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: prometheus-cluster-reader
  roleRef:
    name: cluster-reader
  subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: "${NAMESPACE}"

- apiVersion: authorization.openshift.io/v1
  kind: RoleBinding
  metadata:
    name: prometheus-reader
    namespace: "${NAMESPACE}"
  roleRef:
    name: view
  subjects:
  - kind: ServiceAccount
    name: prometheus-reader
    namespace: "${NAMESPACE}"

# Create a fully end-to-end TLS connection to the prometheus proxy
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    to:
      name: prometheus
    tls:
      termination: Reencrypt
      insecureEdgeTerminationPolicy: Redirect
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/scheme: https
      service.alpha.openshift.io/serving-cert-secret-name: prometheus-tls
    labels:
      name: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    ports:
    - name: prometheus
      port: 443
      protocol: TCP
      targetPort: 8443
    selector:
      app: prometheus

- apiVersion: v1
  kind: Secret
  metadata:
    name: prometheus-proxy
    namespace: "${NAMESPACE}"
  stringData:
    session_secret: "${SESSION_SECRET}="

#Create Persistent Volume Claim    
- apiVersion: "v1"
  kind: "PersistentVolumeClaim"
  metadata:
    name: prometheus-storage
    namespace: "${NAMESPACE}"
  spec:
    accessModes:
      - ${STORAGE_ACCSESS_MODE}
    resources:
      requests:
        storage: ${STORAGE_SIZE}

- apiVersion: extensions/v1beta1
  kind: Deployment
  metadata:
    labels:
      app: prometheus
    name: prometheus
    namespace: "${NAMESPACE}"
  spec:
    updateStrategy:
      type: RollingUpdate
    podManagementPolicy: Parallel
    selector:
      matchLabels:
        app: prometheus
    template:
      metadata:
        labels:
          app: prometheus
        name: prometheus
      spec:
        serviceAccountName: prometheus
        containers:
        # Deploy Prometheus behind an oauth proxy
        - name: prom-proxy
          image: ${IMAGE_PROXY}
          imagePullPolicy: IfNotPresent
          ports:
          - containerPort: 8443
            name: web
          args:
          - -provider=openshift
          - -https-address=:8443
          - -http-address=
          - -email-domain=*
          - -upstream=http://localhost:9090
          - -client-id=system:serviceaccount:${NAMESPACE}:prometheus
          - -openshift-ca=/etc/pki/tls/cert.pem
          - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - '-openshift-sar={"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}'
          - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get", "resourceName": "${NAMESPACE}", "namespace": "${NAMESPACE}"}}'
          - -tls-cert=/etc/tls/private/tls.crt
          - -tls-key=/etc/tls/private/tls.key
          - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
          - -cookie-secret-file=/etc/proxy/secrets/session_secret
          - -skip-auth-regex=^/metrics
          volumeMounts:
          - mountPath: /etc/tls/private
            name: prometheus-tls-secret
          - mountPath: /etc/proxy/secrets
            name: prometheus-proxy-secret
          - mountPath: /prometheus
            name: prometheus-data

        - name: prometheus
          args:
          - --storage.tsdb.retention=6h
          - --config.file=/etc/prometheus/prometheus.yml
          - --web.listen-address=localhost:9090
          image: ${IMAGE_PROMETHEUS}
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/bash
              - -c
              - |-
                set -euo pipefail;
                touch /tmp/prometheusconfig.hash;
                if [[ $(find /etc/prometheus -type f | sort | xargs md5sum | md5sum) != $(cat /tmp/prometheusconfig.hash) ]]; then
                  find /etc/prometheus -type f | sort | xargs md5sum | md5sum > /tmp/prometheusconfig.hash;
                  kill -HUP 1;
                fi
            initialDelaySeconds: 60
            periodSeconds: 60
          volumeMounts:
          - mountPath: /etc/prometheus
            name: prometheus-config
          - mountPath: /prometheus
            name: prometheus-data
          - mountPath: /var/run/secrets/kubernetes.io/scraper
            name: prometheus-scraper-secret


        restartPolicy: Always
        volumes:

        - name: prometheus-config
          configMap:
            defaultMode: 420
            name: prometheus
        - name: prometheus-scraper-secret
          secret:
            secretName: prometheus-scraper
        - name: prometheus-proxy-secret
          secret:
            secretName: prometheus-proxy
        - name: prometheus-tls-secret
          secret:
            secretName: prometheus-tls
        - name: prometheus-data
          persistentVolumeClaim:
            claimName: prometheus-storage

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: prometheus
    namespace: "${NAMESPACE}"
  data:

    prometheus.yml: |
      # rule_files:
      #   - '*.rules'


      scrape_configs:
      - job_name: prometheus
        static_configs:
        - targets:
          - localhost:9090

      - job_name: kubernetes-apiservers
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - action: keep
          regex: default;kubernetes;https
          source_labels:
          - __meta_kubernetes_namespace
          - __meta_kubernetes_service_name
          - __meta_kubernetes_endpoint_port_name
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: kubernetes-nodes-kubelet
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: kubernetes-nodes-cadvisor
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __metrics_path__
          replacement: /metrics/cadvisor
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      - job_name: kubernetes-service-endpoints
          
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - action: keep
          regex: true
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_scrape
        - action: replace
          regex: (https?)
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_scheme
          target_label: __scheme__
        - action: replace
          regex: (.+)
          source_labels:
          - __meta_kubernetes_service_annotation_prometheus_io_path
          target_label: __metrics_path__
        - action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          source_labels:
          - __address__
          - __meta_kubernetes_service_annotation_prometheus_io_port
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - action: replace
          source_labels:
          - __meta_kubernetes_namespace
          target_label: kubernetes_namespace
        - action: replace
          source_labels:
          - __meta_kubernetes_service_name
          target_label: kubernetes_name
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      # - job_name: kubernetes-services
      #   kubernetes_sd_configs:
      #   - role: service
      #   metrics_path: /probe
      #   params:
      #     module:
      #     - http_2xx
      #   relabel_configs:
      #   - action: keep
      #     regex: true
      #     source_labels:
      #     - __meta_kubernetes_service_annotation_prometheus_io_probe
      #   - source_labels:
      #     - __address__
      #     target_label: __param_target
      #   - replacement: blackbox
      #     target_label: __address__
      #   - source_labels:
      #     - __param_target
      #     target_label: instance
      #   - action: labelmap
      #     regex: __meta_kubernetes_service_label_(.+)
      #   - source_labels:
      #     - __meta_kubernetes_namespace
      #     target_label: kubernetes_namespace
      #   - source_labels:
      #     - __meta_kubernetes_service_name
      #     target_label: kubernetes_name

      # - job_name: kubernetes-pods
      #   kubernetes_sd_configs:
      #   - role: pod
      #   relabel_configs:
      #   - action: keep
      #     regex: true
      #     source_labels:
      #     - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      #   - action: replace
      #     regex: (.+)
      #     source_labels:
      #     - __meta_kubernetes_pod_annotation_prometheus_io_path
      #     target_label: __metrics_path__
      #   - action: replace
      #     regex: ([^:]+)(?::\d+)?;(\d+)
      #     replacement: $1:$2
      #     source_labels:
      #     - __address__
      #     - __meta_kubernetes_pod_annotation_prometheus_io_port
      #     target_label: __address__
      #   - action: labelmap
      #     regex: __meta_kubernetes_pod_label_(.+)
      #   - action: replace
      #     source_labels:
      #     - __meta_kubernetes_namespace
      #     target_label: kubernetes_namespace
      #   - action: replace
      #     source_labels:
      #     - __meta_kubernetes_pod_name
      #     target_label: kubernetes_pod_name

      # alerting:
      #   alertmanagers:
      #   - kubernetes_sd_configs:
      #       - role: pod
      #     tls_config:
      #       ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      #     bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      #     relabel_configs:
      #     - source_labels: [__meta_kubernetes_namespace]
      #       regex: kube-system
      #       action: keep
      #     - source_labels: [__meta_kubernetes_pod_label_k8s_app]
      #       regex: alertmanager
      #       action: keep
      #     - source_labels: [__meta_kubernetes_pod_container_port_number]
      #       regex:
      #       action: drop

      # - job_name: 'kubernetes-cadvisor'
      #   scheme: https
      #   tls_config:
      #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      #     insecure_skip_verify: true
      #   bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

      #   metrics_path: /metrics/cadvisor

      #   kubernetes_sd_configs:
      #   - role: node

      #   # Exclude a set of high cardinality metrics that can contribute to significant
      #   # memory use in large clusters. These can be selectively enabled as necessary
      #   # for medium or small clusters.
      #   metric_relabel_configs:
      #   - source_labels: [__name__]
      #     action: drop
      #     regex: 'container_(cpu_user_seconds_total|cpu_cfs_periods_total|memory_usage_bytes|memory_swap|memory_cache|last_seen|fs_(read_seconds_total|write_seconds_total|sector_(.*)|io_(.*)|reads_merged_total|writes_merged_total)|tasks_state|memory_failcnt|memory_failures_total|spec_memory_swap_limit_bytes|fs_(.*)_bytes_total|spec_(.*))'

      #   relabel_configs:
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)

      # - job_name: 'kubernetes-service-endpoints'

      #   tls_config:
      #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      #     # TODO: this should be per target
      #     insecure_skip_verify: true

      #   kubernetes_sd_configs:
      #   - role: endpoints

      #   relabel_configs:
      #   - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
      #     action: keep
      #     regex: true
      #   - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
      #     action: replace
      #     target_label: __scheme__
      #     regex: (https?)
      #   - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
      #     action: replace
      #     target_label: __metrics_path__
      #     regex: (.+)
      #   - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
      #     action: replace
      #     target_label: __address__
      #     regex: (.+)(?::\d+);(\d+)
      #     replacement: $1:$2
      #   - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_username]
      #     action: replace
      #     target_label: __basic_auth_username__
      #     regex: (.+)
      #   - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_password]
      #     action: replace
      #     target_label: __basic_auth_password__
      #     regex: (.+)
      #   - action: labelmap
      #     regex: __meta_kubernetes_service_label_(.+)
      #   - source_labels: [__meta_kubernetes_namespace]
      #     action: replace
      #     target_label: kubernetes_namespace
      #   - source_labels: [__meta_kubernetes_service_name]
      #     action: replace
      #     target_label: kubernetes_name

      # # Scrape config for node-exporter, which is expected to be running on port 9100.
      # - job_name: 'kubernetes-nodes-exporter'
      #   tls_config:
      #     ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      #   kubernetes_sd_configs:
      #   - role: node
      #   metric_relabel_configs:
      #   - source_labels: [__name__]
      #     action: drop
      #     regex: 'node_cpu|node_(disk|scrape_collector)_.+'
      #   # preserve a subset of the network, netstat, vmstat, and filesystem series
      #   - source_labels: [__name__]
      #     action: replace
      #     regex: '(node_(netstat_Ip_.+|vmstat_(nr|thp)_.+|filesystem_(free|size|device_error)|network_(transmit|receive)_(drop|errs)))'
      #     target_label: __name__
      #     replacement: renamed_$1
      #   - source_labels: [__name__]
      #     action: drop
      #     regex: 'node_(netstat|vmstat|filesystem|network)_.+'
      #   - source_labels: [__name__]
      #     action: replace
      #     regex: 'renamed_(.+)'
      #     target_label: __name__
      #     replacement: $1
      #   # drop any partial expensive series
      #   - source_labels: [__name__, device]
      #     action: drop
      #     regex: 'node_network_.+;veth.+'
      #   - source_labels: [__name__, mountpoint]
      #     action: drop
      #     regex: 'node_filesystem_(free|size|device_error);([^/].*|/.+)'
      #   relabel_configs:
      #   - source_labels: [__address__]
      #     regex: '(.*):10250'
      #     replacement: '${1}:9100'
      #     target_label: __address__
      #   - source_labels: [__meta_kubernetes_node_label_kubernetes_io_hostname]
      #     target_label: __instance__
      #   - action: labelmap
      #     regex: __meta_kubernetes_node_label_(.+)


- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: prometheus-node-exporter
    namespace: "${NAMESPACE}"
  # You must grant hostaccess via: oc adm policy add-scc-to-user -z prometheus-node-exporter hostaccess
  # in order for the node-exporter to access the host network and mount /proc and /sys from the host

- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/scrape: "true"
    labels:
      app: prometheus-node-exporter
    name: prometheus-node-exporter
    namespace: "${NAMESPACE}"
  spec:
    clusterIP: None
    ports:
    - name: scrape
      port: 9100
      protocol: TCP
      targetPort: 9100
    selector:
      app: prometheus-node-exporter

- apiVersion: extensions/v1beta1
  kind: DaemonSet
  metadata:
    name: prometheus-node-exporter
    namespace: "${NAMESPACE}"
    labels:
      app: prometheus-node-exporter
      role: monitoring
  spec:
    updateStrategy:
      type: RollingUpdate
    template:
      metadata:
        labels:
          app: prometheus-node-exporter
          role: monitoring
        name: prometheus-exporter
      spec:
        serviceAccountName: prometheus-node-exporter
        hostNetwork: true
        hostPID: true
        containers:
        - image: openshift/prometheus-node-exporter:v0.16.0
          args:
          - "--path.procfs=/host/proc"
          - "--path.sysfs=/host/sys"
          name: node-exporter
          ports:
          - containerPort: 9100
            name: scrape
          resources:
            requests:
              memory: 30Mi
              cpu: 100m
            limits:
              memory: 50Mi
              cpu: 200m
          volumeMounts:
          - name: proc
            readOnly:  true
            mountPath: /host/proc
          - name: sys
            readOnly: true
            mountPath: /host/sys
        volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys

#kube-state-metrics

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
    labels:
      k8s-app: kube-state-metrics
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
      version: v1.3.0
  spec:
    selector:
      matchLabels:
        k8s-app: kube-state-metrics
        version: v1.3.0
    replicas: 1
    template:
      metadata:
        labels:
          k8s-app: kube-state-metrics
          version: v1.3.0
      spec:
        priorityClassName: system-cluster-critical
        serviceAccountName: kube-state-metrics
        containers:
        - name: kube-state-metrics
          image: quay.io/coreos/kube-state-metrics:v1.3.0
          ports:
          - name: http-metrics
            containerPort: 8080
          - name: telemetry
            containerPort: 8081
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 5
            timeoutSeconds: 5
        - name: addon-resizer
          image: k8s.gcr.io/addon-resizer:1.8.6
          resources:
            limits:
              cpu: 100m
              memory: 30Mi
            requests:
              cpu: 100m
              memory: 30Mi
          env:
            - name: MY_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: MY_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
          command:
            - /pod_nanny
            - --config-dir=/etc/config
            - --container=kube-state-metrics
            - --cpu=100m
            - --extra-cpu=1m
            - --memory=100Mi
            - --extra-memory=2Mi
            - --threshold=5
            - --deployment=kube-state-metrics
        volumes:
          - name: config-volume
            configMap:
              name: kube-state-metrics-config

  # Config map for resource configuration.
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: kube-state-metrics-config
    namespace: "${NAMESPACE}"
    labels:
      k8s-app: kube-state-metrics
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  data:
    NannyConfiguration: |-
      apiVersion: nannyconfig/v1alpha1
      kind: NannyConfiguration

- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile

- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  metadata:
    name: kube-state-metrics
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  rules:
  - apiGroups: [""]
    resources:
    - configmaps
    - secrets
    - nodes
    - pods
    - services
    - resourcequotas
    - replicationcontrollers
    - limitranges
    - persistentvolumeclaims
    - persistentvolumes
    - namespaces
    - endpoints
    verbs: ["list", "watch"]
  - apiGroups: ["extensions"]
    resources:
    - daemonsets
    - deployments
    - replicasets
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources:
    - statefulsets
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
    - cronjobs
    - jobs
    verbs: ["list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
    - horizontalpodautoscalers
    verbs: ["list", "watch"]

- apiVersion: rbac.authorization.k8s.io/v1
  kind: Role
  metadata:
    name: kube-state-metrics-resizer
    namespace: "${NAMESPACE}"
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  rules:
  - apiGroups: [""]
    resources:
    - pods
    verbs: ["get"]
  - apiGroups: ["extensions"]
    resources:
    - deployments
    resourceNames: ["kube-state-metrics"]
    verbs: ["get", "update"]

- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: kube-state-metrics
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: kube-state-metrics
  subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: "${NAMESPACE}"

- apiVersion: rbac.authorization.k8s.io/v1
  kind: RoleBinding
  metadata:
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: Role
    name: kube-state-metrics-resizer
  subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: "${NAMESPACE}"

- apiVersion: v1
  kind: Service
  metadata:
    name: kube-state-metrics
    namespace: "${NAMESPACE}"
    labels:
      kubernetes.io/cluster-service: "true"
      addonmanager.kubernetes.io/mode: Reconcile
      kubernetes.io/name: "kube-state-metrics"
    annotations:
      prometheus.io/scrape: 'true'
  spec:
    ports:
    - name: http-metrics
      port: 8080
      targetPort: http-metrics
      protocol: TCP
    - name: telemetry
      port: 8081
      targetPort: telemetry
      protocol: TCP
    selector:
      k8s-app: kube-state-metrics

#   #alertmanager
# - apiVersion: v1
#   kind: PersistentVolumeClaim
#   metadata:
#     name: alertmanager
#     namespace: "${NAMESPACE}"
#     labels:
#       kubernetes.io/cluster-service: "true"
#       addonmanager.kubernetes.io/mode: EnsureExists
#   spec:
#     storageClassName: standard
#     accessModes:
#       - ReadWriteOnce
#     resources:
#       requests:
#         storage: "2Gi"

# - apiVersion: v1
#   kind: ConfigMap
#   metadata:
#     name: alertmanager-config
#     namespace: "${NAMESPACE}"
#     labels:
#       kubernetes.io/cluster-service: "true"
#       addonmanager.kubernetes.io/mode: EnsureExists
#   data:
#     alertmanager.yml: |
#       global: null
#       receivers:
#       - name: default-receiver
#       route:
#         group_interval: 5m
#         group_wait: 10s
#         receiver: default-receiver
#         repeat_interval: 3h

# - apiVersion: apps/v1
#   kind: Deployment
#   metadata:
#     name: alertmanager
#     namespace: "${NAMESPACE}"
#     labels:
#       k8s-app: alertmanager
#       kubernetes.io/cluster-service: "true"
#       addonmanager.kubernetes.io/mode: Reconcile
#       version: v0.14.0
#   spec:
#     replicas: 1
#     selector:
#       matchLabels:
#         k8s-app: alertmanager
#         version: v0.14.0
#     template:
#       metadata:
#         labels:
#           k8s-app: alertmanager
#           version: v0.14.0
#       spec:
#         priorityClassName: system-cluster-critical
#         containers:
#           - name: prometheus-alertmanager
#             image: "prom/alertmanager:v0.14.0"
#             imagePullPolicy: "IfNotPresent"
#             args:
#               - --config.file=/etc/config/alertmanager.yml
#               - --storage.path=/data
#               - --web.external-url=/
#             ports:
#               - containerPort: 9093
#             readinessProbe:
#               httpGet:
#                 path: /#/status
#                 port: 9093
#               initialDelaySeconds: 30
#               timeoutSeconds: 30
#             volumeMounts:
#               - name: config-volume
#                 mountPath: /etc/config
#               - name: storage-volume
#                 mountPath: "/data"
#                 subPath: ""
#             resources:
#               limits:
#                 cpu: 10m
#                 memory: 50Mi
#               requests:
#                 cpu: 10m
#                 memory: 50Mi
#           - name: prometheus-alertmanager-configmap-reload
#             image: "jimmidyson/configmap-reload:v0.1"
#             imagePullPolicy: "IfNotPresent"
#             args:
#               - --volume-dir=/etc/config
#               - --webhook-url=http://localhost:9093/-/reload
#             volumeMounts:
#               - name: config-volume
#                 mountPath: /etc/config
#                 readOnly: true
#             resources:
#               limits:
#                 cpu: 10m
#                 memory: 10Mi
#               requests:
#                 cpu: 10m
#                 memory: 10Mi
#         volumes:
#           - name: config-volume
#             configMap:
#               name: alertmanager-config
#           - name: storage-volume
#             persistentVolumeClaim:
#               claimName: alertmanager

# - apiVersion: v1
#   kind: Service
#   metadata:
#     name: alertmanager
#     namespace: "${NAMESPACE}"
#     labels:
#       kubernetes.io/cluster-service: "true"
#       addonmanager.kubernetes.io/mode: Reconcile
#       kubernetes.io/name: "Alertmanager"
#   spec:
#     ports:
#       - name: http
#         port: 80
#         protocol: TCP
#         targetPort: 9093
#     selector:
#       k8s-app: alertmanager
#     type: "ClusterIP"